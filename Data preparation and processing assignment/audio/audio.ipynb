{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2XB4ecHvS0y"
   },
   "source": [
    "# **Unveiling Emotions: A Comprehensive Analysis of Emotional Speech Data using Advanced Machine Learning Techniques**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVkyqQ2dvX7L"
   },
   "source": [
    "In a world where machines are increasingly interwoven into the fabric of our daily lives, understanding and interpreting human emotions is a frontier that technology is only beginning to navigate. In this exploration, we dive deep into the realm of emotional speech analysis, employing a powerful arsenal of machine learning tools, including the remarkable GPT-4, to unravel the complexities hidden in human vocal expressions.\n",
    "\n",
    "Our journey begins with the meticulous unpacking of a rich dataset, filled with diverse emotional utterances. With careful hands and a discerning eye, we curate the data, ensuring its integrity and readiness for the voyage ahead.\n",
    "\n",
    "Through exploratory data analysis, we seek to uncover the initial secrets held by our dataset. Visualizing, listening, and understanding the nature of the audio files, we prepare ourselves for the more profound analysis lying ahead.\n",
    "\n",
    "As we delve further, the path leads us to the doors of feature extraction. Here, we apply sophisticated techniques to distill valuable characteristics from the raw audio filesâ€”transforming waves into a symphony of features like MFCCs, Chroma, and more.\n",
    "\n",
    "With a well-prepared dataset, our expedition advances into the realms of model building. Guided by the wisdom of GPT-4 and various other machine learning algorithms, we embark on a quest to construct models capable of recognizing and interpreting the emotional essence captured in speech.\n",
    "\n",
    "The saga unfolds as our models, forged in the crucibles of training, are tested and evaluated. Their mettle is proven through rigorous assessments, ensuring their readiness to triumph in real-world applications.\n",
    "\n",
    "Join us in this captivating tale of discovery, where technology meets emotion, and where machines learn to understand the subtle art of human expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyWhv4zwetDx",
    "outputId": "bb8a1b56-945f-4df3-e768-31be150d694a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Acted Emotional Speech Dynamic Database', '__MACOSX']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Path to the zip file\n",
    "zip_file_path = '/content/Acted Emotional Speech Dynamic Database 2.zip'\n",
    "\n",
    "# Directory to unzip the contents\n",
    "unzip_dir = '/content/unzipped_data/'\n",
    "\n",
    "# Creating the directory if it doesn't exist\n",
    "os.makedirs(unzip_dir, exist_ok=True)\n",
    "\n",
    "# Unzipping the file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(unzip_dir)\n",
    "\n",
    "# Listing the contents of the unzipped directory\n",
    "os.listdir(unzip_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szQD_d3oihOV",
    "outputId": "e0a3389f-7879-42f4-f93e-07ab0325abfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happiness', 'anger', '.DS_Store', 'fear', 'sadness', 'disgust']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the \"Acted Emotional Speech Dynamic Database\" directory\n",
    "aesdd_dir = os.path.join(unzip_dir, 'Acted Emotional Speech Dynamic Database')\n",
    "\n",
    "# Listing the contents of the \"Acted Emotional Speech Dynamic Database\" directory\n",
    "os.listdir(aesdd_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNO419B6imEz"
   },
   "source": [
    "The \"Acted Emotional Speech Dynamic Database\" directory contains folders corresponding to different emotions such as 'fear', 'sadness', 'happiness', 'anger', and 'disgust'. It seems like these folders might contain audio files or other types of data related to the respective emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXGiEn4Fin4x",
    "outputId": "f06b146e-b41f-4a97-95db-d6643f89869d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a03 (1).wav',\n",
       " 'a04 (6).wav',\n",
       " 'a02 (4).wav',\n",
       " 'a01 (2).wav',\n",
       " 'a01 (6).wav',\n",
       " 'a05 (4).wav',\n",
       " 'a01 (1).wav',\n",
       " 'a01 (3).wav',\n",
       " 'a03 (5).wav',\n",
       " 'a05 (2).wav']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's explore the 'anger' directory as an example\n",
    "anger_dir = os.path.join(aesdd_dir, 'anger')\n",
    "\n",
    "# Listing the contents of the 'anger' directory\n",
    "os.listdir(anger_dir)[:10]  # Displaying the first 10 files/folders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hifx5oZGitVK"
   },
   "source": [
    "The 'anger' directory contains audio files in WAV format, and there seems to be a naming convention for these files. Each file appears to be named with a prefix (e.g., 'a01', 'a02', etc.) and a number in parentheses, possibly indicating different versions or takes of the emotional expression.\n",
    "\n",
    "To proceed with the exploratory data analysis (EDA), we could:\n",
    "\n",
    "Count the number of audio files in each emotion directory to understand the distribution of data across different emotions.\n",
    "Listen to a few audio files to understand the nature of the emotional expressions.\n",
    "If there are any accompanying metadata or annotation files, we could analyze them to extract additional information about the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxb-GDRyivVD",
    "outputId": "0f926706-9736-40a2-cdd9-c28efd14920c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happiness': 30, 'anger': 30, 'fear': 29, 'sadness': 33, 'disgust': 33}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of audio files in each emotion directory\n",
    "emotion_dirs = os.listdir(aesdd_dir)\n",
    "\n",
    "# Filtering out system files like '.DS_Store'\n",
    "emotion_dirs = [dir for dir in emotion_dirs if not dir.startswith('.')]\n",
    "\n",
    "# Dictionary to hold the count of audio files for each emotion\n",
    "emotion_file_counts = {}\n",
    "\n",
    "for emotion in emotion_dirs:\n",
    "    emotion_path = os.path.join(aesdd_dir, emotion)\n",
    "    # Listing all files in the directory\n",
    "    files = os.listdir(emotion_path)\n",
    "    # Filtering out system files and counting only the audio files (wav)\n",
    "    audio_files = [file for file in files if file.endswith('.wav')]\n",
    "    emotion_file_counts[emotion] = len(audio_files)\n",
    "\n",
    "emotion_file_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zbXiggyHizhD"
   },
   "source": [
    "The dataset contains audio files distributed across different emotions as follows:\n",
    "\n",
    "Fear: 29 audio files\n",
    "Sadness: 33 audio files\n",
    "Happiness: 30 audio files\n",
    "Anger: 30 audio files\n",
    "Disgust: 33 audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MfBtbznqix8z",
    "outputId": "06e6fdf0-04ac-4373-f7f5-5111dd019d93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happiness': '/mnt/data/selected_audio/h04 (1).wav',\n",
       " 'anger': '/mnt/data/selected_audio/a03 (1).wav',\n",
       " 'fear': '/mnt/data/selected_audio/f04 (2).wav',\n",
       " 'sadness': '/mnt/data/selected_audio/s02 (6).wav',\n",
       " 'disgust': '/mnt/data/selected_audio/d03 (01).wav'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import shutil\n",
    "\n",
    "# Directory to save selected audio files\n",
    "selected_audio_dir = '/mnt/data/selected_audio/'\n",
    "\n",
    "# Creating the directory if it doesn't exist\n",
    "os.makedirs(selected_audio_dir, exist_ok=True)\n",
    "\n",
    "# Selecting one audio file from each emotion directory and copying it to the selected_audio directory\n",
    "selected_audio_files = {}\n",
    "for emotion in emotion_dirs:\n",
    "    emotion_path = os.path.join(aesdd_dir, emotion)\n",
    "    # Listing all files in the directory and filtering out system files\n",
    "    files = [file for file in os.listdir(emotion_path) if file.endswith('.wav')]\n",
    "    # Selecting the first audio file\n",
    "    selected_file = files[0] if files else None\n",
    "    if selected_file:\n",
    "        # Copying the selected audio file to the selected_audio directory\n",
    "        shutil.copy(os.path.join(emotion_path, selected_file), selected_audio_dir)\n",
    "        selected_audio_files[emotion] = os.path.join(selected_audio_dir, selected_file)\n",
    "\n",
    "selected_audio_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZBw5ZC8ksTW"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_basic_features(audio_file):\n",
    "    \"\"\"\n",
    "    Extract basic features such as duration and raw audio data amplitude.\n",
    "    \"\"\"\n",
    "    # Loading the audio file with PyDub\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "\n",
    "    # Getting duration\n",
    "    duration = len(audio)\n",
    "\n",
    "    # Getting the raw audio data amplitude\n",
    "    samples = audio.get_array_of_samples()\n",
    "    mean_amplitude = sum(samples) / len(samples)\n",
    "\n",
    "    return duration, mean_amplitude\n",
    "\n",
    "# Directory where your audio files are located\n",
    "\n",
    "\n",
    "# Dataframe to hold the features and labels\n",
    "feature_df = pd.DataFrame(columns=['duration', 'mean_amplitude', 'emotion'])\n",
    "\n",
    "# Emotions to consider\n",
    "emotion_dirs = ['fear', 'sadness', 'happiness', 'anger', 'disgust']\n",
    "\n",
    "# Extracting features from each audio file and adding it to the dataframe\n",
    "for emotion in emotion_dirs:\n",
    "    emotion_path = os.path.join(aesdd_dir, emotion)\n",
    "    # Listing all files in the directory and filtering out system files\n",
    "    files = [file for file in os.listdir(emotion_path) if file.endswith('.wav')]\n",
    "    for file in files:\n",
    "        duration, mean_amplitude = extract_basic_features(os.path.join(emotion_path, file))\n",
    "        feature_df = feature_df.append({\n",
    "            'duration': duration,\n",
    "            'mean_amplitude': mean_amplitude,\n",
    "            'emotion': emotion}, ignore_index=True)\n",
    "\n",
    "# Displaying the head of the dataframe\n",
    "feature_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FybFL3vbt7Rj"
   },
   "source": [
    "This code snippet will load each audio file, extract its duration and the mean amplitude of the raw audio data, and store these features along with the emotion labels in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAgdm4mauYUx"
   },
   "outputs": [],
   "source": [
    "def zero_crossing_rate(samples, frame_length=2048, hop_length=512, sample_rate=44100):\n",
    "    \"\"\"\n",
    "    Calculate the zero-crossing rate of an audio signal.\n",
    "    \"\"\"\n",
    "    zero_crossings = librosa.zero_crossings(samples, pad=False)\n",
    "    return sum(zero_crossings)\n",
    "\n",
    "# Updating the feature extraction function to include zero_crossing_rate\n",
    "def extract_more_features(audio_file):\n",
    "    \"\"\"\n",
    "    Extract more features including duration, mean amplitude, and zero-crossing rate.\n",
    "    \"\"\"\n",
    "    # Loading the audio file with PyDub\n",
    "    audio = AudioSegment.from_wav(audio_file)\n",
    "\n",
    "    # Getting duration\n",
    "    duration = len(audio)\n",
    "\n",
    "    # Getting the raw audio data amplitude\n",
    "    samples = np.array(audio.get_array_of_samples())\n",
    "    mean_amplitude = sum(samples) / len(samples)\n",
    "\n",
    "    # Getting zero-crossing rate\n",
    "    zcr = zero_crossing_rate(samples)\n",
    "\n",
    "    return duration, mean_amplitude, zcr\n",
    "\n",
    "# Updating the dataframe columns\n",
    "feature_df = pd.DataFrame(columns=['duration', 'mean_amplitude', 'zcr', 'emotion'])\n",
    "\n",
    "# Extracting features from each audio file and adding it to the dataframe\n",
    "for emotion in emotion_dirs:\n",
    "    emotion_path = os.path.join(aesdd_dir, emotion)\n",
    "    # Listing all files in the directory and filtering out system files\n",
    "    files = [file for file in os.listdir(emotion_path) if file.endswith('.wav')]\n",
    "    for file in files:\n",
    "        duration, mean_amplitude, zcr = extract_more_features(os.path.join(emotion_path, file))\n",
    "        feature_df = feature_df.append({\n",
    "            'duration': duration,\n",
    "            'mean_amplitude': mean_amplitude,\n",
    "            'zcr': zcr,\n",
    "            'emotion': emotion}, ignore_index=True)\n",
    "\n",
    "# Displaying the head of the dataframe\n",
    "feature_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DxDilv3ua1d"
   },
   "source": [
    "This code calculates the zero-crossing rate and adds it as a feature. You can continue adding more features and preprocessing steps in a similar way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYgQOLx9uqhL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Preparing the features and labels\n",
    "X = np.array(feature_df['features'].tolist())\n",
    "y = feature_df['emotion']\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Building a Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YdDmIBb6u5Sz"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Creating a Random Forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Performing grid search\n",
    "grid_search = GridSearchCV(estimator=clf,\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
